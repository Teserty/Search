{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(\"stopwords\")\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "import io\n",
    "\n",
    "\n",
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem()\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "#Preprocess function\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    return text\n",
    "lemmer = {}\n",
    "index = {}\n",
    "stop = ['', 'из', 'к', 'к', 'В', 'есть', 'более', 'Вы', 'с', 'При', 'для', 'на', 'на', 'в', 'к', '(до', 'на', 'в', 'и', '|', 'На', 'и', 'и', 'в', 'от', 'до', 'когда', 'была', 'была', 'До', 'она', 'была', 'одной', 'из', 'двух', 'с', 'не', 'были', '|', 'от', 'в', '|',\n",
    "        '|', '|', 'с', '/', 'под', 'В.И.', 'С.', 'с.', '|', ':', '[в', '/', 'под', 'В.', '[', 'и', ';', ':', 'И.', 'и', 'и', 'и', 'и', 'В', 'или', 'в', 'в', 'от', 'между', 'и', 'В', 'в', 'об', 'Вы', 'её.', 'в', 'на', 'с', 'об', 'Вы', 'не', 'Ещё', 'об', 'о', 'как', 'для', \n",
    "        'В','других', 'На', 'других', '/', '/', 'Эта', 'в', 'раз', 'была', 'в', 'по', 'в', 'от', 'с', 'нами', 'о', \"[en]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тут очень длинный процесс токенизации и лемматизации\n",
    "import nltk\n",
    "#nltk.download(\"stopwords\")\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "import io\n",
    "mystem = Mystem()\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "import pymorphy2\n",
    "def preprocess_text_opt(text):\n",
    "    text = text.lower().split(\" \")\n",
    "    tokens = [token for token in text if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    return tokens\n",
    "lemmer = {}\n",
    "index = {}\n",
    "w = []\n",
    "def fun(a, b):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    morph = pymorphy2.MorphAnalyzer(lang='ru')\n",
    "    count = 0\n",
    "    for r in range(a, b):\n",
    "        print(r)\n",
    "        cur = \"\"\n",
    "        with io.open(\"./files/выкачка \"+str(r)+\".txt\", encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                cur =cur +  \" \" + line\n",
    "        cur = cur.replace(\"\\n\", '')\n",
    "        cur = cur.replace(\"}\", \"\")\n",
    "        cur = cur.replace(\"{\", \"\")\n",
    "        cur = cur.replace(\"-\", \"\")\n",
    "        cur = cur.replace(\"—\", \"\")\n",
    "        cur = cur.replace(\"\\n\", \"\")\n",
    "        cur = cur.replace(\"(\", \"\")\n",
    "        cur = cur.replace(\")\", \"\")\n",
    "        cur = cur.replace(\".\", \"\")\n",
    "        cur = cur.replace(\",\", \"\")\n",
    "        cur = cur.replace(\":\", \"\")\n",
    "        cur = cur.replace(\"[\", \"\")\n",
    "        cur = cur.replace(\"]\", \"\")\n",
    "        cur = cur.replace(\"'\", \"\")\n",
    "        cur = cur.replace(\"፡\", '')\n",
    "        cur = cur.replace(\";\", \"\")\n",
    "        cur = cur.replace(\"?\", \"\")\n",
    "        cur = cur.replace('\"', \"\")\n",
    "        cur = cur.replace(\"/\", \"\")\n",
    "        cur = cur.replace(\"_\", \"\")\n",
    "        cur = cur.replace('=', \"\")\n",
    "        cur = cur.replace('&', \"\")\n",
    "        cur = cur.replace('…', \"\")\n",
    "        cur = cur.lower()\n",
    "        words = preprocess_text_opt(cur)\n",
    "        txt = \"\"\n",
    "        for i in words:\n",
    "            txt = txt + i + \" \"\n",
    "        lems = mystem.lemmatize(txt)\n",
    "        words = txt.split(\" \")\n",
    "        lems = [i for i in lems if i != \" \"]\n",
    "        for b in range(0, len(words)):\n",
    "            i = words[b]\n",
    "            w.append(i)\n",
    "            d = morph.parse(i)[0].normal_form\n",
    "            if d != \" \": \n",
    "                value = lemmer.get(d)\n",
    "                if d in lemmer.keys():\n",
    "                    if i not in lemmer[d]:\n",
    "                        lemmer[d].append(i)\n",
    "                else:\n",
    "                    lemmer[d] = [i]\n",
    "                if d in index.keys():\n",
    "                    if r not in index[d]:\n",
    "                        index[d].append(r)\n",
    "                else:\n",
    "                    index[d] = [r]\n",
    "                    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#fun(0, 251)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#lemmer\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Слова\n",
    "with open(\"./work/words.txt\", 'w', encoding='utf-8') as file:\n",
    "    for i in w:\n",
    "        file.write(i+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Для 2-го задания\n",
    "with open(\"./work/lemmer total.txt\", 'w', encoding='utf-8') as file:\n",
    "    for i in lemmer.keys():\n",
    "        file.write(i+\" \")\n",
    "        for w in lemmer[i]:\n",
    "            file.write(w + \" \")\n",
    "        file.write(\"\\n\")\n",
    "#Для следующего задания\n",
    "with open(\"./work/index total.txt\", 'w', encoding='utf-8') as file:\n",
    "    for i in index.keys():\n",
    "        file.write(i+\" \")\n",
    "        for w in index[i]:\n",
    "            file.write(str(w) + \" \")\n",
    "        file.write(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
