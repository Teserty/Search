{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(\"stopwords\")\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "import io\n",
    "\n",
    "\n",
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem()\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "#Preprocess function\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    return text\n",
    "lemmer = {}\n",
    "index = {}\n",
    "stop = ['', 'из', 'к', 'к', 'В', 'есть', 'более', 'Вы', 'с', 'При', 'для', 'на', 'на', 'в', 'к', '(до', 'на', 'в', 'и', '|', 'На', 'и', 'и', 'в', 'от', 'до', 'когда', 'была', 'была', 'До', 'она', 'была', 'одной', 'из', 'двух', 'с', 'не', 'были', '|', 'от', 'в', '|',\n",
    "        '|', '|', 'с', '/', 'под', 'В.И.', 'С.', 'с.', '|', ':', '[в', '/', 'под', 'В.', '[', 'и', ';', ':', 'И.', 'и', 'и', 'и', 'и', 'В', 'или', 'в', 'в', 'от', 'между', 'и', 'В', 'в', 'об', 'Вы', 'её.', 'в', 'на', 'с', 'об', 'Вы', 'не', 'Ещё', 'об', 'о', 'как', 'для', \n",
    "        'В','других', 'На', 'других', '/', '/', 'Эта', 'в', 'раз', 'была', 'в', 'по', 'в', 'от', 'с', 'нами', 'о', \"[en]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тут очень длинный процесс токенизации и лемматизации\n",
    "import nltk\n",
    "#nltk.download(\"stopwords\")\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "import io\n",
    "mystem = Mystem()\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "import pymorphy2\n",
    "def preprocess_text_opt(text):\n",
    "    text = text.lower().split(\" \")\n",
    "    tokens = [token for token in text if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    return tokens\n",
    "lemmer = {}\n",
    "index = {}\n",
    "w = []\n",
    "def fun(a, b):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    morph = pymorphy2.MorphAnalyzer(lang='ru')\n",
    "    count = 0\n",
    "    for r in range(a, b):\n",
    "        print(r)\n",
    "        cur = \"\"\n",
    "        with io.open(\"./files/выкачка \"+str(r)+\".txt\", encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                cur =cur +  \" \" + line\n",
    "        cur = cur.replace(\"\\n\", '')\n",
    "        cur = cur.replace(\"}\", \"\")\n",
    "        cur = cur.replace(\"{\", \"\")\n",
    "        cur = cur.replace(\"-\", \"\")\n",
    "        cur = cur.replace(\"—\", \"\")\n",
    "        cur = cur.replace(\"\\n\", \"\")\n",
    "        cur = cur.replace(\"(\", \"\")\n",
    "        cur = cur.replace(\")\", \"\")\n",
    "        cur = cur.replace(\".\", \"\")\n",
    "        cur = cur.replace(\",\", \"\")\n",
    "        cur = cur.replace(\":\", \"\")\n",
    "        cur = cur.replace(\"[\", \"\")\n",
    "        cur = cur.replace(\"]\", \"\")\n",
    "        cur = cur.replace(\"'\", \"\")\n",
    "        cur = cur.replace(\"፡\", '')\n",
    "        cur = cur.replace(\";\", \"\")\n",
    "        cur = cur.replace(\"?\", \"\")\n",
    "        cur = cur.replace('\"', \"\")\n",
    "        cur = cur.replace(\"/\", \"\")\n",
    "        cur = cur.replace(\"_\", \"\")\n",
    "        cur = cur.replace('=', \"\")\n",
    "        cur = cur.replace('&', \"\")\n",
    "        cur = cur.replace('…', \"\")\n",
    "        cur = cur.lower()\n",
    "        words = preprocess_text_opt(cur)\n",
    "        txt = \"\"\n",
    "        for i in words:\n",
    "            txt = txt + i + \" \"\n",
    "        words = txt.split(\" \")\n",
    "        lems = [i for i in lems if i != \" \"]\n",
    "        for b in range(0, len(words)):\n",
    "            i = words[b]\n",
    "            w.append(i)\n",
    "            d = morph.parse(i)[0].normal_form\n",
    "            if d != \" \": \n",
    "                value = lemmer.get(d)\n",
    "                if d in lemmer.keys():\n",
    "                    if i not in lemmer[d]:\n",
    "                        lemmer[d].append(i)\n",
    "                else:\n",
    "                    lemmer[d] = [i]\n",
    "                if d in index.keys():\n",
    "                    if r not in index[d]:\n",
    "                        index[d].append(r)\n",
    "                else:\n",
    "                    index[d] = [r]\n",
    "                    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#fun(0, 251)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#lemmer\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Слова\n",
    "with open(\"./work/words.txt\", 'w', encoding='utf-8') as file:\n",
    "    for i in w:\n",
    "        file.write(i+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b1e70cd87cad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Для 2-го задания\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./work/lemmer total.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlemmer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lemmer' is not defined"
     ]
    }
   ],
   "source": [
    "#Для 2-го задания\n",
    "with open(\"./work/lemmer total.txt\", 'w', encoding='utf-8') as file:\n",
    "    for i in lemmer.keys():\n",
    "        file.write(i+\" \")\n",
    "        for w in lemmer[i]:\n",
    "            file.write(w + \" \")\n",
    "        file.write(\"\\n\")\n",
    "#Для следующего задания\n",
    "with open(\"./work/index total.txt\", 'w', encoding='utf-8') as file:\n",
    "    for i in index.keys():\n",
    "        file.write(i+\" \")\n",
    "        for w in index[i]:\n",
    "            file.write(str(w) + \" \")\n",
    "        file.write(\"\\n\")\n",
    "with open(\"./Tasks/task2/words.txt\", 'w', encoding='utf-8') as file:\n",
    "    for i in set(w):\n",
    "        file.write(i+\"\\n\")\n",
    "with open(\"./Tasks/task2/wordslist.txt\", 'w', encoding='utf-8') as file:\n",
    "    for i in w:\n",
    "        file.write(i+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640698\n",
      "320349\n"
     ]
    }
   ],
   "source": [
    "#Задание 4\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "morph = pymorphy2.MorphAnalyzer(lang='ru')\n",
    "w = []\n",
    "with open(\"./Tasks/task2/words.txt\", 'r', encoding='utf-8') as file:\n",
    "    for i in file:\n",
    "        i = i.replace(\"\\n\", \"\")\n",
    "        w.append(i)\n",
    "        \n",
    "print(len(w))\n",
    "print(len(set(w)))\n",
    "\n",
    "import collections\n",
    "import math \n",
    "c = collections.Counter()\n",
    "for word in w:\n",
    "     c[morph.parse(word)[0].normal_form] += 1\n",
    "        \n",
    "tf = dict()\n",
    "idf = dict()\n",
    "tf_idf = dict()\n",
    "for i in c:\n",
    "    tf[i] = c[i]/len(w)\n",
    "\n",
    "with open(\"./work/index total.txt\", 'r', encoding='utf-8') as file:\n",
    "    for i in file:\n",
    "        i = i.replace(\"\\n\", \"\")\n",
    "        a = i.split(\" \")\n",
    "        #Пока жесткий код\n",
    "        idf[a[0]] = math.log(251/(len(a)-1))\n",
    "for i in tf.keys():\n",
    "    tf_idf[i] = tf[i]*idf[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Tasks/task4/TF_IDF.txt\", 'w', encoding='utf-8') as file:\n",
    "    for i in tf.keys():\n",
    "        file.write(i+\" \"+ str(idf[i]) +\" \"+ str(tf_idf[i]) +\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
